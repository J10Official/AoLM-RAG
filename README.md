# NPTEL Knowledge Graph RAG System

Jatin Agrawal (202314014)
Ayush Kumar Gupta (2023114001)

**Github link**: https://github.com/J10Official/AoLM-RAG


---

## 1. Project Scope

This project implements a **Graph-based RAG system for personalized learning** that serves two primary functions:

1. **Study Planning** â€” Uses LLM-powered concept extraction to build a knowledge graph, enabling intelligent prerequisite recommendations and personalized learning paths.

2. **Continuous Evaluation** â€” Employs deterministic mastery tracking based on psychological forgetting curves, eliminating the need for traditional examinations while providing real-time assessment of student understanding.

### Core Capabilities

| Feature | Description |
|---------|-------------|
| **Mastery Evaluation** | Tracks student understanding using psychological forgetting curvesâ€”enabling continuous assessment without traditional exams |
| **Prerequisite Recommendations** | Identifies which lectures to review based on concept dependencies and current retention levels |
| **Cross-Course Mastery Transfer** | Mastery of concepts carries over between courses that share the same concepts |
| **Personalized Learning Paths** | Recommends optimal study sequences based on individual knowledge state |

### RAG Architecture Flow

```
Student Profile  â†’  RETRIEVE  â†’  AUGMENT  â†’  GENERATE  â†’  Study Plan
      â”‚               â”‚            â”‚            â”‚             â”‚
      â”‚         Query graph    Add mastery   Algorithmic    Personalized
      â”‚         for relevant   state and     recommendation  output
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€conceptsâ”€â”€â”€â”€â”€â”€â”€decay ratesâ”€â”€â”€â”€engineâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Step 1 â€” Retrieval**: Query the knowledge graph to find concepts related to the student's goals (e.g., prerequisites for a target course).

**Step 2 â€” Augmentation**: Enrich retrieved concepts with the student's mastery data (retention levels, review history, decay rates).

**Step 3 â€” Generation**: Produce personalized study plans using an algorithmic recommendation engineâ€”deterministically, without requiring LLM inference at runtime.

> **Design Note**: LLMs are used exclusively during the **data pipeline phase** to extract concepts from lecture transcripts. The runtime recommendation engine relies on graph traversal and mathematical models, ensuring reproducible and explainable outputs.

---

## 2. Methodology

This system implements a **Graph-based Retrieval-Augmented Generation (RAG)** architecture for NPTEL (National Programme on Technology Enhanced Learning) courses. The system extracts knowledge from course materials, constructs a hierarchical knowledge graph, and provides continuous student assessment through psychological forgetting curve modelsâ€”replacing traditional examination-based evaluation.

### Key Metrics

| Metric | Value |
|--------|-------|
| Total Nodes | 5,082 |
| Total Edges | 7,257 |
| Unique Concepts | 4,417 |
| Lectures Processed | 645 |
| Courses Covered | 18 |

### Highlights
- Interactive visualization with mastery-based coloring
- Student retention modeling using Ebbinghaus forgetting curve
- Deterministic evaluation without traditional exams

---

## 3. System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA SOURCES                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Excel Course   â”‚  â”‚  NPTEL Website  â”‚  â”‚ YouTube Videos  â”‚  â”‚
â”‚  â”‚     List        â”‚  â”‚  (Playwright)   â”‚  â”‚  (Transcripts)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                    â”‚                    â”‚
            â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     EXTRACTION PIPELINE                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚Excel Parser â”‚â†’ â”‚NPTEL Scraperâ”‚â†’ â”‚  Concept    â”‚              â”‚
â”‚  â”‚             â”‚  â”‚  (async)    â”‚  â”‚  Extractor  â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  (LLM+async)â”‚              â”‚
â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      KNOWLEDGE GRAPH                            â”‚
â”‚                                                                 â”‚
â”‚   V_stream â”€â”€HAS_COURSEâ”€â”€â–º V_course â”€â”€HAS_LECTUREâ”€â”€â–º V_lecture  â”‚
â”‚                                                          â”‚      â”‚
â”‚                                               COVERS_CONCEPT    â”‚
â”‚                                                          â”‚      â”‚
â”‚                                                          â–¼      â”‚
â”‚                                                     V_concept   â”‚
â”‚                                                                 â”‚
â”‚   Storage: JSONL (nodes.jsonl, edges.jsonl)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APPLICATION LAYER                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Visualizer    â”‚        â”‚      Student Mastery Model      â”‚ â”‚
â”‚  â”‚   (pyvis)       â”‚        â”‚   (Ebbinghaus Forgetting Curve) â”‚ â”‚
â”‚  â”‚                 â”‚        â”‚                                 â”‚ â”‚
â”‚  â”‚ â€¢ Force-directedâ”‚        â”‚ R(t) = e^(-t/S)                 â”‚ â”‚
â”‚  â”‚ â€¢ Hierarchical  â”‚        â”‚ â€¢ Retention tracking            â”‚ â”‚
â”‚  â”‚ â€¢ Radial        â”‚        â”‚ â€¢ Spaced repetition             â”‚ â”‚
â”‚  â”‚ â€¢ Mastery colorsâ”‚        â”‚ â€¢ Progress visualization        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Data Collection, Cleaning & Challenges

### Overview

Building this knowledge graph required collecting and processing data from three distinct sources, each with unique challenges. This section documents our experience with data acquisition, cleaning procedures, and representative examples of the issues encountered.

### 4.1 Data Source 1: Excel Course List

**Source:** `Final Course List (Jan - Apr 2026).xlsx` provided by NPTEL

**What We Extracted:**
- Course names and IDs
- Stream/discipline categorization
- Professor names and institutes

**Challenges Encountered:**

| Challenge | Description | Solution |
|-----------|-------------|----------|
| Inconsistent Naming | Same stream named differently ("CS", "Computer Science", "Comp. Sci.") | Regex normalization + manual slug mapping |
| Merged Cells | Excel used merged cells for stream categories | Custom pandas parsing with `merge_cells=False` |
| Missing Data | Some courses lacked NPTEL IDs | Cross-referenced with NPTEL website |
| Encoding Issues | Professor names with special characters (Ã¼, Ã±) | UTF-8 encoding with fallback |

**Representative Example - Cleaning Stream Names:**
```python
# Raw data from Excel
"Comp. Sci. & Engg."
"Computer Science and Engineering" 
"CS & Engineering"
"Computer Sc."

# After normalization â†’ All become:
"Computer Science and Engineering"
```

### 4.2 Data Source 2: NPTEL Website Scraping

**Source:** https://nptel.ac.in/courses/{course_id}

**What We Extracted:**
- Course abstracts and descriptions
- Lecture titles organized by week
- YouTube video URLs for each lecture

**Challenges Encountered:**

| Challenge | Description | Solution |
|-----------|-------------|----------|
| Dynamic JavaScript Rendering | Course pages load content via JS | Used Playwright (headless browser) instead of simple requests |
| Rate Limiting | NPTEL blocks rapid requests | Implemented delays (1-2s between requests) + rotating user agents |
| Inconsistent HTML Structure | Different courses had different DOM layouts | Wrote multiple CSS selector fallbacks |
| Missing Lectures | Some courses had "Coming Soon" placeholders | Skip detection + graceful handling |
| Session Expiry | Long scraping sessions got blocked | Periodic browser restart every 50 courses |

**Representative Example - Handling Missing YouTube URLs:**
```python
# Raw scraped data
lecture_1: "https://www.youtube.com/watch?v=abc123"
lecture_2: "Coming Soon"  # No video yet
lecture_3: ""  # Empty string
lecture_4: "https://youtu.be/def456"  # Different URL format

# After cleaning
lecture_1: "https://www.youtube.com/watch?v=abc123"  âœ“
lecture_2: SKIPPED (no video available)
lecture_3: SKIPPED (empty URL)  
lecture_4: "https://www.youtube.com/watch?v=def456"  âœ“ (normalized)
```

**Scraping Statistics:**
- Total pages scraped: 18 course pages + 645 lecture pages
- Success rate: 94.2% (some lectures had no videos)
- Time taken: ~45 minutes with politeness delays

### 4.3 Data Source 3: YouTube Transcript Extraction

**Source:** YouTube auto-generated captions via `youtube-transcript-api`

**What We Extracted:**
- Full lecture transcripts (text + timestamps)
- Language detection for multi-language courses

**Challenges Encountered:**

| Challenge | Description | Solution |
|-----------|-------------|----------|
| No Captions Available | ~8% of videos had no auto-captions | Skip and log; extract concepts from title only |
| Hindi/Regional Languages | Some lectures in Hindi with English captions | Prioritize English captions, fall back to Hindi |
| Caption Quality | Auto-generated captions have errors (e.g., "neural network" â†’ "neural net work") | LLM extraction is robust to minor errors |
| Timestamp Fragmentation | Captions come in 3-5 word chunks | Concatenate into coherent paragraphs |
| Video Unavailable | Some videos were private/deleted | Graceful skip with logging |

**Representative Example - Transcript Cleaning:**
```
# Raw caption fragments from YouTube API
[0.0s] "hello and welcome to"
[1.2s] "this lecture on"
[2.1s] "neural networks we will"
[3.5s] "discuss back propagation"

# After concatenation and cleaning
"Hello and welcome to this lecture on neural networks. 
 We will discuss back propagation..."
```

**Transcript Statistics:**
- Videos processed: 645
- Successful extractions: 594 (92.1%)
- Failed (no captions): 51
- Average transcript length: ~4,500 words per lecture

### 4.4 Data Source 4: LLM Concept Extraction

**Source:** OpenRouter API with free-tier models

**What We Extracted:**
- Atomic concepts (single knowledge units)
- Concept descriptions
- Keywords for each concept

**Challenges Encountered:**

| Challenge | Description | Solution |
|-----------|-------------|----------|
| Rate Limiting | Free models have strict rate limits (20 req/min) | Model rotation across 3 providers + exponential backoff |
| Inconsistent Output Format | LLMs sometimes returned malformed JSON | Regex-based JSON extraction + retry logic |
| Duplicate Concepts | Same concept extracted with slightly different names | Fuzzy deduplication (Levenshtein distance < 3) |
| Over-Extraction | LLM extracting irrelevant concepts ("uh", "okay") | Prompt engineering + post-filtering |
| Context Window Limits | Long transcripts exceed context limits | Chunk transcripts into 2000-word segments |

**Representative Example - Handling LLM Rate Limits:**
```
Request 1 â†’ Model A (deepseek) â†’ Success
Request 2 â†’ Model A (deepseek) â†’ 429 Rate Limited
Request 3 â†’ Model B (qwen) â†’ Success  â† Automatic failover
Request 4 â†’ Model B (qwen) â†’ 429 Rate Limited
Request 5 â†’ Model C (llama) â†’ Success â† Automatic failover
[Wait 60 seconds]
Request 6 â†’ Model A (deepseek) â†’ Success â† Back to primary
```

**Representative Example - Concept Deduplication:**
```
# Raw extracted concepts (before deduplication)
"Artificial Intelligence"
"artificial intelligence"
"AI"
"A.I."
"Artficial Inteligence"  # Typo from poor transcript

# After deduplication â†’ Single canonical concept:
{
  "id": "concept_artificial_intelligence",
  "name": "Artificial Intelligence",
  "aliases": ["AI", "A.I."]
}
```

### 4.5 Summary: Data Pipeline Statistics

| Stage | Input | Output | Success Rate | Time |
|-------|-------|--------|--------------|------|
| Excel Parsing | 1 file | 18 courses | 100% | <1s |
| NPTEL Scraping | 18 courses | 645 lectures | 94.2% | ~45 min |
| Transcript Fetch | 645 videos | 594 transcripts | 92.1% | ~30 min |
| Concept Extraction | 594 transcripts | 4,417 concepts | 87.3% | ~2 hours |

**Total Pipeline Time:** ~3 hours for full extraction

### 4.6 Data Quality Assurance

After extraction, we performed validation checks:

```bash
$ python -m src.validate data/output

Validation Results:
  âœ“ All nodes have required fields (id, type, name)
  âœ“ All edges reference valid source/target nodes
  âœ“ No orphan nodes (every concept linked to at least 1 lecture)
  âœ“ No duplicate node IDs
  âš  23 concepts have very short descriptions (<10 chars) - flagged for review
```

---

## 5. Graph Schema

### Node Types

| Type | Count | Description |
|------|-------|-------------|
| `V_stream` | 2 | Academic discipline categories |
| `V_course` | 18 | NPTEL courses |
| `V_lecture` | 645 | Individual video lectures |
| `V_concept` | 4,417 | Atomic knowledge concepts |

### Edge Types

| Type | Count | Description |
|------|-------|-------------|
| `HAS_COURSE` | 18 | Stream â†’ Course relationship |
| `HAS_LECTURE` | 645 | Course â†’ Lecture relationship |
| `COVERS_CONCEPT` | 6,594 | Lecture â†’ Concept relationship |

### Sample Node Structures

**Stream Node:**
```json
{
  "id": "stream_computer_science_and_engineering",
  "type": "V_stream",
  "name": "Computer Science and Engineering",
  "properties": { "slug": "computer_science_and_engineering" }
}
```

**Course Node:**
```json
{
  "id": "course_106108840",
  "type": "V_course",
  "name": "Foundations of Deep Learning: Concepts and Applications",
  "properties": {
    "course_id": "noc26-cs01",
    "nptel_url": "https://nptel.ac.in/courses/106108840",
    "professor": "Prof. Sriram Ganapathy...",
    "institute": "IISc Bangalore",
    "duration": "12 Weeks"
  }
}
```

**Lecture Node:**
```json
{
  "id": "lecture_106108840_w1_l1",
  "type": "V_lecture",
  "name": "Overview of Machine Learning and Deep Learning",
  "properties": {
    "week": 1,
    "lecture_num": 1,
    "youtube_url": "https://www.youtube.com/watch?v=SThBzCPRmYg",
    "course_id": "course_106108840"
  }
}
```

**Concept Node:**
```json
{
  "id": "concept_artificial_intelligence",
  "type": "V_concept",
  "name": "Artificial Intelligence",
  "properties": {
    "description": "A field focused on creating intelligent machines...",
    "keywords": ["ai", "machine learning", "intelligence"],
    "source_lectures": ["lecture_106108840_w1_l1", ...]
  }
}
```

---

## 6. Module Inventory

| Module | Lines | Purpose |
|--------|-------|---------|
| `visualizer.py` | 1,039 | Interactive HTML graph visualization with pyvis |
| `pipeline.py` | 647 | Unified orchestration pipeline |
| `student_mastery.py` | 601 | Ebbinghaus forgetting curve implementation |
| `async_concept_extractor.py` | 536 | Parallel LLM concept extraction |
| `graph_builder.py` | 396 | JSONL graph construction |
| `concept_extractor.py` | 374 | Single-threaded concept extraction |
| `nptel_scraper.py` | 318 | Playwright-based web scraping |
| `extract_concepts.py` | 312 | Concept extraction orchestration |
| `validate.py` | 227 | Graph validation utilities |
| `transcript_fetcher.py` | 214 | YouTube transcript extraction |
| `main.py` | 184 | Legacy main entry point |
| `excel_parser.py` | 123 | NPTEL course list parser |

**Total Codebase:** ~5,000 lines of Python

---

## 7. Student Mastery Model

### Ebbinghaus Forgetting Curve

The system models student memory using the classic forgetting curve:

$$R(t) = e^{-t/S}$$

Where:
- **R** = Retention probability (0 to 1)
- **t** = Time elapsed since last review (days)
- **S** = Memory stability (increases with successful reviews)

### Memory Stability Growth

After each successful review, stability increases:

$$S_{new} = S_{base} \times (1 + D \times S_{old}^{-decay})$$

Where:
- **S_new** = Updated stability after review
- **S_base** = Base stability multiplier (default: 2.0)
- **D** = Review quality factor (1-5 scale, based on recall difficulty)
- **S_old** = Previous stability value
- **decay** = Diminishing returns exponent (default: 0.3)

#### Understanding Memory Stability

Memory stability (S) represents **how resistant a memory is to forgetting**. It can be interpreted as the time constant in the exponential decayâ€”specifically, the number of days until retention drops to ~37% (1/e).

**Key Insights:**

1. **Initial Stability is Low**: When a student first learns a concept, S â‰ˆ 1.0 day. This means after just one day without review, retention drops to ~37%. This aligns with Ebbinghaus's original findings that most forgetting happens within 24 hours.

2. **Stability Grows with Quality Reviews**: Each successful review increases stability. The growth is governed by:
   - **Quality Score (1-5)**: How well the student recalled the concept
     - 1-2: Poor recall (stability may decrease)
     - 3: Moderate recall (small stability increase)
     - 4-5: Good recall (significant stability increase)

3. **Diminishing Returns**: The `stability_decay` parameter (0.3) ensures that as stability grows, additional reviews provide progressively smaller benefits. This models the real phenomenon where well-learned material requires less frequent review.

4. **Spacing Effect**: The formula naturally implements the spacing effect from cognitive psychology. Concepts with low stability need frequent review; those with high stability can be reviewed less often.

#### Numerical Example: Stability Growth Over Time

| Review # | Days Since Last | Quality | Old Stability | New Stability | Retention at Review |
|----------|-----------------|---------|---------------|---------------|---------------------|
| 1 (Initial) | â€” | â€” | â€” | 1.0 days | 100% |
| 2 | 1 day | 4.0 | 1.0 | 2.8 days | 37% |
| 3 | 3 days | 4.5 | 2.8 | 5.2 days | 34% |
| 4 | 7 days | 4.0 | 5.2 | 8.1 days | 26% |
| 5 | 14 days | 4.5 | 8.1 | 11.8 days | 18% |
| 6 | 30 days | 5.0 | 11.8 | 16.4 days | 8% |

After 6 reviews over ~2 months, the student can retain the concept for over 2 weeks at >50% retentionâ€”compared to just 1 day initially.

#### Why This Matters for Exam-Free Assessment

Traditional exams create artificial "snapshot" assessments. Our model provides:

- **Continuous Assessment**: Every interaction updates mastery estimates
- **Decay Awareness**: We know when mastery is slipping before an exam would reveal it
- **Personalized Intervals**: Each student's optimal review schedule is computed individually
- **Concept-Level Granularity**: Not "passed the course" but "mastered 847/1000 concepts at >80% retention"

### Parameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| Initial Stability | 1.0 day | Time until 50% forgotten |
| Stability Growth | 2.0Ã— | Multiplier per successful review |
| Mastery Threshold | 0.8 | Retention level for "mastered" |
| Stability Decay | 0.3 | Diminishing returns factor |

### Retention Color Mapping

| Retention Range | Color Gradient | Meaning |
|-----------------|----------------|---------|
| 0.0 | Gray (#95a5a6) | Not studied |
| 0.0 - 0.4 | Gray â†’ Red | Struggling/Forgotten |
| 0.4 - 0.7 | Red â†’ Yellow | Needs review |
| 0.7 - 1.0 | Yellow â†’ Green | Mastered |

---

## 8. Visualization Features

### Interactive Features

- **Zoom/Pan:** Mouse wheel and drag
- **Node Hover:** Detailed tooltips with properties
- **Toggle Toolbar:** Hide controls for screenshots
- **Mastery Legend:** Color key for retention levels
- **Physics Freeze:** Auto-freeze for performance (large graphs)

### Generated Visualizations

| File | Nodes | Description |
|------|-------|-------------|
| `graph_force_all.html` | 5,082 | Full knowledge graph to get a quick overview of data |
| `graph_force_student.html` | 52 | Student subgraph with mastery colors |

> **View Interactive Visualizations:** The full interactive HTML visualizations can be viewed on GitHub by opening the files `graph_force_all.html` and `graph_force_student.html` in the repository root folder.

### Full Knowledge Graph (`graph_force_all.html`)

The complete knowledge graph shows all 5,082 nodes organized by type. Nodes are colored by their type in the hierarchy:

**Node Type Legend:**

| Node Type | Color | Count |
|-----------|-------|-------|
| Stream | Purple (`#9b59b6`) | 2 |
| Course | Blue (`#3498db`) | 18 |
| Lecture | Orange (`#e67e22`) | 645 |
| Concept | Green (`#2ecc71`) | 4,417 |

![Full Knowledge Graph - Zoomed Out](images/all%20zoomed%20out.png)

*Figure: Full knowledge graph (zoomed out) showing the overall structure with all 5,082 nodes*

![Full Knowledge Graph - Zoomed In](images/all%20zoomed%20in.png)

*Figure: Full knowledge graph (zoomed in) showing node details and connections*

---

### Student Mastery Graph (`graph_force_student.html`)

The student mastery view shows only the concepts the student has studied, with nodes colored by retention level using the Ebbinghaus forgetting curve model.

![Student Mastery Graph - Zoomed Out](images/mastery%20zoomed%20out.png)

*Figure: Student subgraph (zoomed out) showing mastery-based coloring for studied concepts*

![Student Mastery Graph - Zoomed In](images/mastery%20zoomed%20in.png)

*Figure: Student subgraph (zoomed in) showing individual concept retention levels*

**Mastery Color Legend:**

![Mastery Legend](images/Legend%20mastery.png)

---

## 9. Data Pipeline

### Execution Flow

```bash
python -m src.pipeline --api-key <KEY> \
    --stream "Computer Science and Engineering" \
    --stream "Mathematics" \
    --concurrency 15
```

### Pipeline Stages

1. **Excel Parsing** - Parse course list from Excel file
2. **NPTEL Scraping** - Fetch lecture metadata and YouTube URLs
3. **Transcript Fetching** - Extract transcripts from YouTube
4. **Concept Extraction** - LLM-powered concept identification
5. **Graph Building** - Construct JSONL graph files

### Performance

- **Concurrency:** Up to 20 parallel API requests
- **Rate Limiting:** Automatic backoff with model rotation
- **Models Used:** 3 free OpenRouter models for redundancy

---

## 10. Dependencies

```
# Core
pandas>=2.0.0          # Excel handling
openpyxl>=3.1.0        # Excel file format

# Web Scraping
requests>=2.31.0       # HTTP client
beautifulsoup4>=4.12.0 # HTML parsing
playwright>=1.40.0     # Dynamic content scraping

# Visualization
pyvis                  # Interactive network graphs

# Transcripts
youtube-transcript-api # YouTube caption extraction

# Utilities
tqdm>=4.66.0          # Progress bars
aiohttp               # Async HTTP for parallel extraction
```

---

## 11. File Structure

```
RAG System/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipeline.py          # Main entry point
â”‚   â”œâ”€â”€ excel_parser.py      # Course list parser
â”‚   â”œâ”€â”€ nptel_scraper.py     # Web scraper
â”‚   â”œâ”€â”€ transcript_fetcher.py # YouTube transcripts
â”‚   â”œâ”€â”€ concept_extractor.py  # LLM extraction
â”‚   â”œâ”€â”€ async_concept_extractor.py # Parallel extraction
â”‚   â”œâ”€â”€ graph_builder.py     # Graph construction
â”‚   â”œâ”€â”€ visualizer.py        # HTML visualization
â”‚   â”œâ”€â”€ student_mastery.py   # Forgetting curve model
â”‚   â””â”€â”€ validate.py          # Validation utilities
â”œâ”€â”€ data/
â”‚   â””â”€â”€ output/
â”‚       â”œâ”€â”€ nodes.jsonl      # 5,082 nodes
â”‚       â”œâ”€â”€ edges.jsonl      # 7,257 edges
â”‚       â””â”€â”€ student_profiles.json
â”œâ”€â”€ graph_force_all.html     # Full graph visualization
â”œâ”€â”€ graph_force_student.html # Student mastery view
â””â”€â”€ requirements.txt
```

---

## 12. Usage Examples

### Generate Full Pipeline
```bash
python -m src.pipeline --api-key <KEY> --stream "Computer Science"
```

### Visualization Only
```bash
# Full graph
python -m src.visualizer -o graph.html

# Single course
python -m src.visualizer --course "Machine Learning" --max-concepts 100

# With student mastery
python -m src.visualizer --generate-sample --mastery-only -o student.html
```

### Validate Graph
```bash
python -m src.validate data/output
```

---

## 13. Challenges and Limitations

### Scalability Constraints

The current implementation faces significant scalability challenges:

| Challenge | Description | Impact |
|-----------|-------------|--------|
| **Exponential Graph Growth** | Nodes and edges grow exponentially with course additions | Processing 18 courses produced 5,082 nodes and 7,257 edges; scaling to 100+ courses may exceed memory limits |
| **High Node Density** | Even a modest dataset generates substantial graph complexity | 645 lectures yielded 4,417 conceptsâ€”averaging ~7 concepts per lecture |
| **API Cost Escalation** | LLM-based concept extraction incurs costs per lecture | Extracting concepts from thousands of lectures becomes prohibitively expensive with paid APIs |
| **Rate Limiting** | Free-tier API models impose strict request limits | Pipeline execution time increases significantly due to mandatory backoff periods |

### Computational Limitations

- **Visualization Performance**: Graphs exceeding 10,000 nodes require physics freezing; real-time interaction degrades beyond 50,000 nodes
- **Memory Footprint**: Full graph loaded in memory; no streaming or pagination support
- **Single-Machine Architecture**: No distributed processing for large-scale deployments

### Data Quality Constraints

- **Transcript Dependency**: Concept extraction quality depends heavily on YouTube auto-caption accuracy
- **LLM Variability**: Different models may extract inconsistent concepts from identical content
- **Deduplication Limits**: Fuzzy matching may miss semantically equivalent concepts with different phrasing

---

## 14. Future Extensions

### Planned Enhancements

| Priority | Enhancement | Description |
|----------|-------------|-------------|
| High | **Interaction-Based Mastery Updates** | Update mastery values in real-time based on student interactions with course content (video watch time, quiz attempts, revisits) |
| High | **Neo4j Integration** | Migrate from JSONL to Neo4j for efficient graph queries and traversal |
| Medium | **Natural Language Query Interface** | Enable students to ask questions like "What should I study before Deep Learning?" |
| Medium | **Concept Prerequisite Extraction** | Automatically identify prerequisite relationships between concepts |
| Low | **Multi-Modal Content** | Integrate lecture slides, diagrams, and PDF materials into the knowledge graph |

### Scalability Roadmap

- **Phase 1**: Implement graph database backend (Neo4j/ArangoDB)
- **Phase 2**: Add streaming/pagination for large graph visualization
- **Phase 3**: Distributed concept extraction pipeline

---

## 15. Recommendation Example: Baseline vs Our System

To demonstrate the practical value of our mastery-aware knowledge graph, we present a comparison between a traditional recommendation approach and our system using the sample student "Demo Student" who has studied 2 lectures.

### Student Profile: Demo Student

**Studied Lectures:**
1. "Lec 36: Phases in Split Transaction Bus" (Computer Architecture)
2. "IID Assumption" (Machine Learning/Statistics)

**Concept Mastery State (22 concepts total):**

| Retention Level | Count | Examples |
|-----------------|-------|----------|
| High (0.7-1.0) | 16 | "Bus Architecture", "Pipeline Stages", "IID Property" |
| Medium (0.4-0.7) | 2 | "Transaction Protocol", "Statistical Independence" |
| Low (0.0-0.4) | 4 | "Cache Coherence", "Probability Distribution", "Markov Chain", "Sampling Bias" |

### Scenario: Student Wants to Take "Deep Learning Foundations" Course

The student asks: *"What should I study before starting the Deep Learning course?"*

---

### Baseline Approach: Static Prerequisite List

A traditional system would return a **fixed prerequisite list** based on course metadata:

```
BASELINE RECOMMENDATION:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“š Prerequisites for "Deep Learning Foundations":
  1. Linear Algebra (full course)
  2. Probability and Statistics (full course)
  3. Python Programming (full course)
  4. Calculus (Weeks 1-4)
  
Estimated study time: 12-16 weeks
```

**Problems with Baseline:**
- âŒ Ignores what the student already knows
- âŒ Recommends entire courses even if student knows 80% of material
- âŒ No prioritization based on importance or decay
- âŒ Same recommendation for every student

---

### Our System: Mastery-Aware Personalized Recommendation

Our system traverses the knowledge graph, checks concept mastery, and generates a **targeted recommendation**:

```
OUR SYSTEM RECOMMENDATION:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š Analyzing your mastery state...

âœ… CONCEPTS YOU ALREADY KNOW (can skip):
  â€¢ "IID Property" (94% retention) - covered in Deep Learning Week 2
  â€¢ "Statistical Independence" (62% retention) - covered in Deep Learning Week 2
  â€¢ "Bus Architecture" (91% retention) - not needed for Deep Learning

ğŸ”´ URGENT REVIEW NEEDED (retention < 40%):
  â€¢ "Probability Distribution" (18% retention)
    â””â”€ Required for: Lecture "Probabilistic Models" (Week 3)
    â””â”€ Recommended: Re-watch "IID Assumption" lecture, focus on minutes 12-18
    
  â€¢ "Sampling Bias" (23% retention)  
    â””â”€ Required for: Lecture "Training Data Quality" (Week 5)
    â””â”€ Recommended: Review "Statistical Sampling" (new lecture suggestion)

ğŸŸ¡ CONCEPTS TO LEARN (not yet studied):
  Priority-ordered by number of dependent lectures in Deep Learning:
  
  1. "Gradient Descent" (prerequisite for 12 lectures)
     â””â”€ Suggested: "Optimization Fundamentals" - Lecture 4, Week 2
     
  2. "Matrix Multiplication" (prerequisite for 9 lectures)
     â””â”€ Suggested: "Linear Algebra Review" - Lecture 1, Week 1
     
  3. "Backpropagation" (prerequisite for 8 lectures)
     â””â”€ Suggested: "Neural Network Training" - Lecture 7, Week 3
     â””â”€ Note: Requires "Gradient Descent" first

ğŸ“ˆ PERSONALIZED STUDY PLAN:
  Week 1: Matrix Multiplication, Gradient Descent (4 hours)
  Week 2: Review Probability Distribution, Backpropagation (3 hours)
  Week 3: Begin Deep Learning course (ready for Weeks 1-3)
  
Estimated preparation time: 2 weeks (vs 12-16 weeks baseline)
Time saved: 10-14 weeks by skipping known material
```

---

### Comparison Summary

| Aspect | Baseline | Our System |
|--------|----------|------------|
| **Personalization** | None (same for all students) | Fully personalized based on mastery state |
| **Granularity** | Course-level | Concept-level |
| **Considers Decay** | No | Yes (identifies urgent reviews) |
| **Study Time Estimate** | 12-16 weeks | 2 weeks |
| **Prerequisite Source** | Static metadata | Dynamic graph traversal |
| **Cross-Course Transfer** | Not supported | Automatic (IID concept carries over) |
| **Actionable Guidance** | "Take course X" | "Watch lecture Y, minutes 12-18" |

### How the Recommendation is Generated

```
Algorithm: GeneratePrerequisiteRecommendation(student, target_course)

1. Get all concepts covered by target_course lectures
2. For each concept:
   a. Check if concept exists in student's mastery profile
   b. If yes and retention > 0.7: Mark as "already known"
   c. If yes and retention < 0.4: Mark as "urgent review"
   d. If no: Mark as "to learn"
3. For "to learn" concepts:
   a. Find lectures that cover this concept
   b. Count how many target_course lectures depend on it
   c. Sort by dependency count (most important first)
4. Generate study plan with optimal ordering
```

This demonstrates how the knowledge graph enables **intelligent, personalized recommendations** that a traditional prerequisite system cannot provide.

---

## 16. Statistics Summary

| Metric | Value |
|--------|-------|
| Total Nodes | 5,082 |
| Total Edges | 7,257 |
| Streams | 2 |
| Courses | 18 |
| Lectures | 645 |
| Unique Concepts | 4,417 |
| Concepts per Lecture | ~6.8 avg |
| Codebase Size | ~5,000 lines |
| Python Modules | 12 |

---

*Report generated by the NPTEL Knowledge Graph RAG System*
